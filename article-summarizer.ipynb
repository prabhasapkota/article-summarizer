{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Prabha Sapkota\n",
    "\n",
    "### GitHub Repo: https://github.com/prabhasapkota/article-summarizer\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "annotated-types    0.6.0\n",
      "appnope            0.1.4\n",
      "asttokens          2.4.1\n",
      "beautifulsoup4     4.12.3\n",
      "blis               0.7.11\n",
      "catalogue          2.0.10\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "click              8.1.7\n",
      "cloudpathlib       0.16.0\n",
      "comm               0.2.2\n",
      "confection         0.1.4\n",
      "contourpy          1.2.1\n",
      "cycler             0.12.1\n",
      "cymem              2.0.8\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "en-core-web-sm     3.7.1\n",
      "executing          2.0.1\n",
      "fonttools          4.51.0\n",
      "idna               3.7\n",
      "ipykernel          6.29.4\n",
      "ipython            8.23.0\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.3\n",
      "joblib             1.4.0\n",
      "jupyter_client     8.6.1\n",
      "jupyter_core       5.7.2\n",
      "kiwisolver         1.4.5\n",
      "langcodes          3.3.0\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib         3.8.4\n",
      "matplotlib-inline  0.1.6\n",
      "murmurhash         1.0.10\n",
      "nest-asyncio       1.6.0\n",
      "nltk               3.8.1\n",
      "numpy              1.26.4\n",
      "packaging          24.0\n",
      "pandas             2.2.2\n",
      "parso              0.8.4\n",
      "pexpect            4.9.0\n",
      "pillow             10.3.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.0\n",
      "preshed            3.0.9\n",
      "prompt-toolkit     3.0.43\n",
      "psutil             5.9.8\n",
      "ptyprocess         0.7.0\n",
      "pure-eval          0.2.2\n",
      "pydantic           2.7.0\n",
      "pydantic_core      2.18.1\n",
      "Pygments           2.17.2\n",
      "pyparsing          3.1.2\n",
      "python-dateutil    2.9.0.post0\n",
      "pytz               2024.1\n",
      "pyzmq              25.1.2\n",
      "regex              2023.12.25\n",
      "requests           2.31.0\n",
      "setuptools         65.5.0\n",
      "six                1.16.0\n",
      "smart-open         6.4.0\n",
      "soupsieve          2.5\n",
      "spacy              3.7.4\n",
      "spacy-legacy       3.0.12\n",
      "spacy-loggers      1.0.5\n",
      "spacytextblob      4.0.0\n",
      "srsly              2.4.8\n",
      "stack-data         0.6.3\n",
      "textblob           0.15.3\n",
      "thinc              8.2.3\n",
      "tornado            6.4\n",
      "tqdm               4.66.2\n",
      "traitlets          5.14.2\n",
      "typer              0.9.4\n",
      "typing_extensions  4.11.0\n",
      "tzdata             2024.1\n",
      "urllib3            2.2.1\n",
      "wasabi             1.1.2\n",
      "wcwidth            0.2.13\n",
      "weasel             0.3.4\n",
      "All prereqs installed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "!pip list\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article HTML content has been saved to 'article.html'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the article\n",
    "url = \"https://www.cbsnews.com/news/how-often-total-solar-eclipses-happen/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Get the HTML content of the webpage\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Save the HTML content to a file\n",
    "    with open(\"article.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "        \n",
    "    print(\"Article HTML content has been saved to 'article.html'\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the article. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content has been saved to 'article.txt'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML content from the saved file\n",
    "with open('article.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML and extract plain text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "article_text = soup.get_text()\n",
    "\n",
    "# Save plain text to a new file\n",
    "with open('article.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(article_text)\n",
    "\n",
    "print(\"Article content has been saved to 'article.txt'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Score: 0.1075241046831956\n",
      "Number of Sentences in the Original Article: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/prabha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download the 'punkt' tokenizer model from NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(\"article.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find the article text using appropriate tags or class names\n",
    "# In this case, let's assume the article text is contained within <p> tags\n",
    "article_paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "# Concatenate the text of all paragraphs to get the full article text\n",
    "article_text = \"\\n\".join([p.get_text() for p in article_paragraphs])\n",
    "\n",
    "# Perform sentiment analysis on the article text\n",
    "article_blob = TextBlob(article_text)\n",
    "polarity_score = article_blob.sentiment.polarity\n",
    "\n",
    "# Print the polarity score with an appropriate label\n",
    "print(\"Polarity Score:\", polarity_score)\n",
    "\n",
    "# Count the number of sentences in the original article\n",
    "num_sentences = len(article_blob.sentences)\n",
    "\n",
    "# Print the number of sentences with an appropriate label\n",
    "print(\"Number of Sentences in the Original Article:\", num_sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    " Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Frequent Tokens (converted to lowercase):\n",
      "eclipse\n",
      "solar\n",
      "total\n",
      "cbs\n",
      "news\n",
      "\n",
      "Token Frequencies:\n",
      "total: 21\n",
      "solar: 26\n",
      "eclipses: 4\n",
      "happen: 2\n",
      "monday: 2\n",
      "rare: 6\n",
      "cbs: 18\n",
      "news: 18\n",
      "latest: 1\n",
      "u.s.: 7\n",
      "world: 2\n",
      "politics: 2\n",
      "entertainment: 2\n",
      "healthwatch: 2\n",
      "moneywatch: 2\n",
      "crime: 2\n",
      "sports: 2\n",
      "local: 1\n",
      "baltimore: 2\n",
      "bay: 2\n",
      "area: 2\n",
      "boston: 2\n",
      "chicago: 2\n",
      "colorado: 2\n",
      "detroit: 2\n",
      "los: 2\n",
      "angeles: 2\n",
      "miami: 2\n",
      "minnesota: 2\n",
      "new: 4\n",
      "york: 4\n",
      "philadelphia: 2\n",
      "pittsburgh: 2\n",
      "sacramento: 2\n",
      "texas: 4\n",
      "live: 3\n",
      "streaming: 1\n",
      "shows: 1\n",
      "48: 1\n",
      "hours: 1\n",
      "60: 1\n",
      "minutes: 1\n",
      "america: 4\n",
      "decides: 1\n",
      "evening: 1\n",
      "mornings: 2\n",
      "eye: 1\n",
      "reports: 1\n",
      "saturday: 1\n",
      "morning: 2\n",
      "dish: 1\n",
      "face: 1\n",
      "nation: 1\n",
      "comes: 1\n",
      "sun: 6\n",
      "person: 2\n",
      "prime: 1\n",
      "time: 2\n",
      "sunday: 1\n",
      "takeout: 1\n",
      "uplift: 1\n",
      "weekender: 1\n",
      "2024: 7\n",
      "eclipse: 28\n",
      "photos: 1\n",
      "podcasts: 1\n",
      "depth: 1\n",
      "newsletters: 1\n",
      "download: 1\n",
      "app: 2\n",
      "team: 2\n",
      "executive: 1\n",
      "paramount: 1\n",
      "shop: 1\n",
      "paramount+: 2\n",
      "join: 1\n",
      "talent: 1\n",
      "community: 1\n",
      "rss: 1\n",
      "feeds: 1\n",
      "moment: 1\n",
      "innovators: 1\n",
      "disruptors: 1\n",
      "essentials: 1\n",
      "watch: 2\n",
      "kerry: 3\n",
      "breen: 3\n",
      "updated: 1\n",
      "april: 5\n",
      "9: 1\n",
      "5:05: 1\n",
      "edt: 2\n",
      "cuts: 2\n",
      "path: 7\n",
      "03:57: 1\n",
      "8: 3\n",
      "drew: 1\n",
      "totality: 7\n",
      "north: 2\n",
      "throwing: 1\n",
      "swaths: 1\n",
      "united: 4\n",
      "states: 5\n",
      "canada: 1\n",
      "mexico: 1\n",
      "darkness: 1\n",
      "middle: 2\n",
      "day: 1\n",
      "occurs: 1\n",
      "moon: 4\n",
      "passes: 2\n",
      "earth: 3\n",
      "blocking: 1\n",
      "view: 3\n",
      "called: 1\n",
      "completely: 1\n",
      "blocks: 1\n",
      "light: 1\n",
      "events: 4\n",
      "viewed: 1\n",
      "specific: 1\n",
      "areas: 2\n",
      "short: 1\n",
      "periods: 1\n",
      "stretched: 1\n",
      "east: 1\n",
      "coast: 1\n",
      "cities: 1\n",
      "including: 2\n",
      "dallas: 1\n",
      "buffalo: 1\n",
      "burlington: 1\n",
      "vermont: 1\n",
      "best: 3\n",
      "views: 1\n",
      "map: 2\n",
      "nasa: 3\n",
      "getty: 1\n",
      "images: 1\n",
      "happen?total: 1\n",
      "ones: 1\n",
      "visible: 5\n",
      "rarer: 1\n",
      "occur: 2\n",
      "years: 1\n",
      "globe: 1\n",
      "poles: 1\n",
      "ocean: 1\n",
      "number: 1\n",
      "factors: 1\n",
      "enjoying: 1\n",
      "viewers: 1\n",
      "need: 1\n",
      "clear: 1\n",
      "skies: 1\n",
      "ensure: 1\n",
      "phenomenon: 2\n",
      "blocked: 1\n",
      "clouds: 1\n",
      "effect: 1\n",
      "blotted: 1\n",
      "outside: 1\n",
      "observe: 1\n",
      "partial: 1\n",
      "covers: 2\n",
      "said: 1\n",
      "places: 1\n",
      "04:04: 1\n",
      "2021: 1\n",
      "antarctica: 1\n",
      "period: 1\n",
      "lasted: 1\n",
      "minute: 1\n",
      "54: 1\n",
      "seconds: 1\n",
      "according: 2\n",
      "noaa: 2\n",
      "2017: 1\n",
      "parts: 1\n",
      "oregon: 1\n",
      "south: 1\n",
      "carolina: 1\n",
      "able: 1\n",
      "witness: 1\n",
      "1979: 1\n",
      "aug.: 1\n",
      "12: 1\n",
      "2026: 1\n",
      "viewable: 1\n",
      "arctic: 1\n",
      "eastern: 1\n",
      "greenland: 1\n",
      "northern: 1\n",
      "spain: 1\n",
      "iceland: 1\n",
      "looking: 1\n",
      "wait: 1\n",
      "significantly: 1\n",
      "longer: 1\n",
      "wo: 1\n",
      "contiguous: 1\n",
      "august: 1\n",
      "2044: 1\n",
      "great: 1\n",
      "american: 1\n",
      "tourists: 1\n",
      "faced: 1\n",
      "heavy: 1\n",
      "traffic: 1\n",
      "long: 1\n",
      "delays: 1\n",
      "driving: 1\n",
      "home: 1\n",
      "space: 1\n",
      "station: 1\n",
      "captures: 1\n",
      "image: 1\n",
      "shadow: 1\n",
      "bonnie: 1\n",
      "tyler: 1\n",
      "hit: 1\n",
      "soars: 1\n",
      "music: 1\n",
      "charts: 1\n",
      "couple: 2\n",
      "gets: 2\n",
      "engaged: 2\n",
      "flight: 2\n",
      "missed: 1\n",
      "video: 1\n",
      "key: 1\n",
      "moments: 1\n",
      "reporter: 1\n",
      "editor: 1\n",
      "cbsnews.com: 1\n",
      "graduate: 1\n",
      "university: 1\n",
      "arthur: 1\n",
      "l.: 1\n",
      "carter: 1\n",
      "school: 1\n",
      "journalism: 1\n",
      "previously: 1\n",
      "worked: 1\n",
      "nbc: 1\n",
      "today: 1\n",
      "digital: 1\n",
      "current: 1\n",
      "breaking: 2\n",
      "issues: 1\n",
      "substance: 1\n",
      "use: 2\n",
      "twitter: 2\n",
      "published: 1\n",
      "6:00: 1\n",
      "©: 2\n",
      "interactive: 2\n",
      "inc.: 2\n",
      "rights: 2\n",
      "reserved: 2\n",
      "3: 1\n",
      "investments: 1\n",
      "consider: 1\n",
      "inflation: 1\n",
      "rising: 1\n",
      "find: 1\n",
      "tax: 1\n",
      "relief: 1\n",
      "company: 1\n",
      "iowa: 1\n",
      "governor: 1\n",
      "signs: 1\n",
      "bill: 1\n",
      "allows: 1\n",
      "arrest: 1\n",
      "migrants: 1\n",
      "copyright: 1\n",
      "privacy: 1\n",
      "policy: 1\n",
      "california: 1\n",
      "notice: 1\n",
      "sell: 1\n",
      "personal: 1\n",
      "information: 1\n",
      "terms: 1\n",
      "advertise: 1\n",
      "closed: 1\n",
      "captioning: 1\n",
      "store: 1\n",
      "site: 1\n",
      "contact: 1\n",
      "help: 1\n",
      "facebook: 1\n",
      "instagram: 1\n",
      "youtube: 1\n",
      "open: 1\n",
      "chrome: 1\n",
      "safari: 1\n",
      "continue: 1\n",
      "know: 1\n",
      "browser: 1\n",
      "notifications: 1\n",
      "exclusive: 1\n",
      "reporting: 1\n",
      "turn: 1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the article text from the .txt file\n",
    "with open(\"article.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    article_text = f.read()\n",
    "\n",
    "# Process the article text with spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Tokenize the text and convert tokens to lowercase\n",
    "tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "# Count the frequency of each token\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# Identify the 5 most frequent tokens\n",
    "most_common_tokens = token_freq.most_common(5)\n",
    "\n",
    "# Print the common tokens with appropriate label\n",
    "print(\"5 Most Frequent Tokens (converted to lowercase):\")\n",
    "for token, freq in most_common_tokens:\n",
    "    print(token)\n",
    "\n",
    "# Print the tokens and their frequencies with appropriate labels\n",
    "print(\"\\nToken Frequencies:\")\n",
    "for token, freq in token_freq.items():\n",
    "    print(f\"{token}: {freq}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Frequent Lemmas:\n",
      "\n",
      "\n",
      "\n",
      ": 78\n",
      "the: 52\n",
      "eclipse: 32\n",
      ",: 30\n",
      ".: 28\n",
      "\n",
      "Lemmas with Frequencies:\n",
      "\n",
      "\n",
      "    : 1\n",
      "how: 4\n",
      "often: 4\n",
      "total: 21\n",
      "solar: 26\n",
      "eclipse: 32\n",
      "happen: 2\n",
      "—: 2\n",
      "and: 15\n",
      "why: 2\n",
      "monday: 2\n",
      "'s: 5\n",
      "be: 24\n",
      "so: 2\n",
      "rare: 6\n",
      "-: 1\n",
      "cbs: 18\n",
      "news: 18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "late: 1\n",
      "\n",
      "\n",
      ": 10\n",
      "u.s.: 7\n",
      "\n",
      "\n",
      "\n",
      ": 78\n",
      "world: 2\n",
      "politics: 2\n",
      "entertainment: 2\n",
      "healthwatch: 2\n",
      "moneywatch: 2\n",
      "crime: 2\n",
      "sports: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 5\n",
      "local: 1\n",
      "baltimore: 2\n",
      "bay: 2\n",
      "area: 4\n",
      "boston: 2\n",
      "chicago: 2\n",
      "colorado: 2\n",
      "detroit: 2\n",
      "los: 2\n",
      "angeles: 2\n",
      "miami: 2\n",
      "minnesota: 2\n",
      "new: 4\n",
      "york: 4\n",
      "philadelphia: 2\n",
      "pittsburgh: 2\n",
      "sacramento: 2\n",
      "texas: 4\n",
      "live: 3\n",
      "streaming: 1\n",
      "show: 1\n",
      "48: 1\n",
      "hour: 1\n",
      "60: 1\n",
      "minutes: 1\n",
      "america: 4\n",
      "decides: 1\n",
      "evening: 1\n",
      "mornings: 2\n",
      "eye: 1\n",
      "on: 10\n",
      "reports: 1\n",
      "saturday: 1\n",
      "morning: 2\n",
      "the: 52\n",
      "dish: 1\n",
      "face: 2\n",
      "nation: 1\n",
      "here: 1\n",
      "come: 1\n",
      "sun: 6\n",
      "person: 2\n",
      "to: 16\n",
      "prime: 1\n",
      "time: 2\n",
      "sunday: 1\n",
      "takeout: 1\n",
      "uplift: 1\n",
      "weekender: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 2\n",
      "2024: 7\n",
      "photos: 1\n",
      "podcast: 1\n",
      "in: 13\n",
      "depth: 1\n",
      "newsletters: 1\n",
      "download: 1\n",
      "our: 3\n",
      "app: 2\n",
      "team: 2\n",
      "executive: 1\n",
      "paramount: 1\n",
      "shop: 1\n",
      "paramount+: 2\n",
      "join: 1\n",
      "talent: 1\n",
      "community: 1\n",
      "rss: 1\n",
      "feed: 1\n",
      "a: 12\n",
      "moment: 2\n",
      "with: 3\n",
      "...: 1\n",
      "innovators: 1\n",
      "&: 1\n",
      "disruptors: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "essential: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "watch: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    : 1\n",
      "by: 2\n",
      "\n",
      "                        \n",
      "              : 1\n",
      "kerry: 3\n",
      "breen: 3\n",
      "update: 1\n",
      ":: 2\n",
      " : 1\n",
      "april: 5\n",
      "9: 1\n",
      ",: 30\n",
      "/: 4\n",
      "5:05: 1\n",
      "am: 2\n",
      "edt: 2\n",
      "\n",
      "          : 2\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "cut: 2\n",
      "path: 7\n",
      "across: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "\n",
      ": 20\n",
      "03:57: 1\n",
      "8: 3\n",
      " : 8\n",
      "draw: 1\n",
      "\": 2\n",
      "of: 23\n",
      "totality: 7\n",
      "over: 1\n",
      "north: 2\n",
      "throw: 1\n",
      "swath: 1\n",
      "united: 4\n",
      "states: 4\n",
      "canada: 1\n",
      "mexico: 1\n",
      "into: 2\n",
      "darkness: 1\n",
      "middle: 2\n",
      "day: 1\n",
      ".: 28\n",
      "occur: 3\n",
      "when: 6\n",
      "moon: 4\n",
      "pass: 2\n",
      "between: 1\n",
      "earth: 3\n",
      "block: 3\n",
      "view: 5\n",
      "as: 1\n",
      "it: 2\n",
      "call: 1\n",
      "completely: 1\n",
      "light: 1\n",
      "these: 1\n",
      "event: 4\n",
      "can: 2\n",
      "only: 4\n",
      "specific: 1\n",
      "for: 5\n",
      "short: 1\n",
      "period: 2\n",
      "stretch: 1\n",
      "from: 9\n",
      "east: 1\n",
      "coast: 1\n",
      "city: 1\n",
      "include: 2\n",
      "dallas: 1\n",
      ";: 2\n",
      "buffalo: 1\n",
      "burlington: 1\n",
      "vermont: 1\n",
      "among: 1\n",
      "good: 3\n",
      "map: 2\n",
      " \n",
      "\n",
      "            \n",
      "                : 1\n",
      "nasa: 3\n",
      "getty: 1\n",
      "images: 1\n",
      "\n",
      "\n",
      "                          \n",
      ": 1\n",
      "do: 2\n",
      "happen?total: 1\n",
      "one: 3\n",
      "that: 3\n",
      "visible: 5\n",
      "even: 1\n",
      "rarer: 1\n",
      "every: 1\n",
      "three: 1\n",
      "year: 1\n",
      "somewhere: 1\n",
      "around: 1\n",
      "globe: 1\n",
      "but: 2\n",
      "pole: 1\n",
      "or: 1\n",
      "ocean: 1\n",
      "number: 1\n",
      "factor: 1\n",
      "go: 1\n",
      "enjoy: 1\n",
      "viewer: 1\n",
      "need: 1\n",
      "clear: 1\n",
      "sky: 1\n",
      "ensure: 1\n",
      "phenomenon: 2\n",
      "not: 5\n",
      "cloud: 1\n",
      "those: 3\n",
      "within: 1\n",
      "will: 6\n",
      "see: 4\n",
      "full: 1\n",
      "effect: 1\n",
      "blot: 1\n",
      "out: 1\n",
      "outside: 1\n",
      "still: 1\n",
      "observe: 1\n",
      "partial: 1\n",
      "cover: 2\n",
      "some: 2\n",
      "all: 3\n",
      "say: 1\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "place: 1\n",
      "04:04: 1\n",
      "last: 5\n",
      "?: 3\n",
      "2021: 1\n",
      "antarctica: 1\n",
      "just: 2\n",
      "minute: 1\n",
      "54: 1\n",
      "second: 1\n",
      "accord: 2\n",
      "noaa: 2\n",
      "2017: 1\n",
      "part: 1\n",
      "oregon: 1\n",
      "south: 1\n",
      "carolina: 1\n",
      "able: 1\n",
      "witness: 1\n",
      "before: 1\n",
      "1979: 1\n",
      "five: 1\n",
      "state: 1\n",
      "next: 2\n",
      "aug.: 1\n",
      "12: 1\n",
      "2026: 1\n",
      "this: 1\n",
      "viewable: 1\n",
      "arctic: 1\n",
      "eastern: 1\n",
      "greenland: 1\n",
      "northern: 1\n",
      "spain: 1\n",
      "iceland: 1\n",
      "look: 1\n",
      "have: 1\n",
      "wait: 1\n",
      "significantly: 1\n",
      "long: 2\n",
      "contiguous: 1\n",
      "until: 1\n",
      "august: 1\n",
      "2044: 1\n",
      "great: 1\n",
      "american: 1\n",
      "\n",
      "\n",
      "              : 1\n",
      "more: 3\n",
      "\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "tourist: 1\n",
      "heavy: 1\n",
      "traffic: 1\n",
      "delay: 1\n",
      "drive: 1\n",
      "home: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 5\n",
      "space: 1\n",
      "station: 1\n",
      "capture: 1\n",
      "image: 1\n",
      "shadow: 1\n",
      "during: 2\n",
      "bonnie: 1\n",
      "tyler: 1\n",
      "hit: 1\n",
      "soar: 1\n",
      "music: 1\n",
      "chart: 1\n",
      "couple: 2\n",
      "get: 3\n",
      "engage: 2\n",
      "flight: 2\n",
      "miss: 1\n",
      "video: 1\n",
      "key: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            : 1\n",
      "\n",
      "            \n",
      ": 1\n",
      "reporter: 1\n",
      "editor: 1\n",
      "at: 2\n",
      "cbsnews.com: 1\n",
      "graduate: 1\n",
      "university: 1\n",
      "arthur: 1\n",
      "l.: 1\n",
      "carter: 1\n",
      "school: 1\n",
      "journalism: 1\n",
      "she: 2\n",
      "previously: 1\n",
      "work: 1\n",
      "nbc: 1\n",
      "': 1\n",
      "today: 1\n",
      "digital: 1\n",
      "current: 1\n",
      "break: 2\n",
      "issue: 1\n",
      "substance: 1\n",
      "use: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                      : 1\n",
      "twitter: 2\n",
      "\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "first: 2\n",
      "publish: 1\n",
      "6:00: 1\n",
      "©: 2\n",
      "interactive: 2\n",
      "inc.: 2\n",
      "rights: 1\n",
      "reserve: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          : 1\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          : 3\n",
      "3: 1\n",
      "investment: 1\n",
      "consider: 1\n",
      "inflation: 1\n",
      "rise: 1\n",
      "find: 1\n",
      "tax: 1\n",
      "relief: 1\n",
      "company: 1\n",
      "iowa: 1\n",
      "governor: 1\n",
      "sign: 1\n",
      "bill: 1\n",
      "allow: 1\n",
      "arrest: 1\n",
      "migrant: 1\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "copyright: 1\n",
      "right: 1\n",
      "privacy: 1\n",
      "policy: 1\n",
      "california: 1\n",
      "notice: 1\n",
      "sell: 1\n",
      "my: 1\n",
      "personal: 1\n",
      "information: 1\n",
      "term: 1\n",
      "about: 1\n",
      "advertise: 1\n",
      "closed: 1\n",
      "captioning: 1\n",
      "store: 1\n",
      "site: 1\n",
      "contact: 1\n",
      "us: 1\n",
      "help: 1\n",
      "facebook: 1\n",
      "instagram: 1\n",
      "youtube: 1\n",
      "open: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": 1\n",
      "chrome: 1\n",
      "safari: 1\n",
      "continue: 1\n",
      "know: 1\n",
      "browser: 1\n",
      "notification: 1\n",
      "exclusive: 1\n",
      "reporting: 1\n",
      "now: 1\n",
      "turn: 1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the article text\n",
    "with open(\"article.txt\", \"r\") as file:\n",
    "    article_text = file.read()\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Extract lemmas and count their frequencies\n",
    "lemma_counter = Counter([token.lemma_.lower() for token in doc])\n",
    "\n",
    "# Get the 5 most common lemmas\n",
    "common_lemmas = lemma_counter.most_common(5)\n",
    "\n",
    "print(\"5 Most Frequent Lemmas:\")\n",
    "for lemma, frequency in common_lemmas:\n",
    "    print(f\"{lemma}: {frequency}\")\n",
    "\n",
    "print(\"\\nLemmas with Frequencies:\")\n",
    "for lemma, frequency in lemma_counter.items():\n",
    "    print(f\"{lemma}: {frequency}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
